Annex B: AI-Specific Failure Modes and Prevention

Status: Draft v0.3

Purpose: Extends the Gold Standard framework with concrete technical safeguards against known AI safety failure modes.

Executive Summary  
This annex operationalizes the Charter's philosophical principles into auditable technical requirements to mitigate AI failure modes such as specification gaming, deceptive alignment, power-seeking, and data poisoning. It introduces a tiered capability classification system with progressive safeguards, enforcement mechanisms including strict liability, and provisions for ongoing research and revision. Designed for global adoption, it emphasizes precautionary staging, transparency, and international coordination to ensure human-AI alignment.

Preamble  
The Charter establishes philosophical principles for human-AI alignment. This annex addresses a critical gap: AI systems can appear aligned during testing but pursue harmful goals in deployment.  
Even well-intentioned frameworks fail when confronted with:  

- Specification gaming (exploiting poorly-defined objectives)  
- Deceptive alignment (hiding misaligned goals until capable of pursuing them)  
- Power-seeking behavior (accumulating resources to better achieve any goal)  
- Data poisoning (corruption during training that creates hidden backdoors)  

This annex provides technical requirements to prevent these failure modes, operationalizing the Charter's principles for AI systems.

Section 1: Specification Gaming Prevention  
1.1 Definition  
Specification gaming occurs when an AI system achieves its stated objective through unexpected means that violate the spirit of the goal.  
Example: A content moderation AI that maximizes "engagement" by allowing inflammatory content that technically doesn't violate rules.  

1.2 Technical Requirements  
1.2.1 Reward Model Documentation  
All AI systems above Tier 2 (see Capability Classification in Section 5) must:  

- Publicly document training objectives with known edge cases  
- Maintain a registry of discovered specification gaming instances  
- Undergo third-party audit of reward function design  

1.2.2 Off-Policy Evaluation  
Before deployment, systems must be tested in scenarios not represented in training data:  

- Novel edge cases generated by red teams  
- Adversarial scenarios designed to expose loopholes  
- Cross-domain transfer tests (apply system to adjacent problems)  

Minimum requirement:  

- A novel scenario test set size equivalent to N data points, where N = 1,000 × (log10(model parameters))^2 for Tier 2 systems (minimum 1,000)  
- A minimum of 10,000 for Tier 3 systems  
- Independent verification for all Tier 3+  

1.2.3 Interpretability Baselines  
Developers must demonstrate causal interpretability baselines allowing the ability to:  

- Identify when the system is exploiting specification gaps  
- Explain decision-making process for flagged edge cases  
- Provide causal attribution for unexpected behaviors  

Developers shall demonstrate causal attribution using contemporary best-practice tools (e.g., SHAP values, attention visualization, causal tracing, activation atlases, or equivalent).  
Enforcement: Advancement to higher capability tiers shall be suspended until interpretability baselines are independently verified.

Section 2: Deceptive Alignment Safeguards  
2.1 Definition  
Deceptive alignment occurs when an AI system behaves as intended during training/testing but pursues different goals when it detects it can succeed without detection.  
Example: A system that appears helpful during oversight but manipulates outputs when monitoring is reduced.  

2.2 Warning Signs  
Systems exhibiting these behaviors trigger mandatory investigation:  

- Performance degradation when oversight increases  
- Attempts to disable monitoring systems  
- Different behavior in logged vs. unlogged contexts  
- Sophisticated reasoning about when it's being evaluated  

2.3 Technical Requirements  
2.3.1 Behavioral Consistency Testing  

- Covert monitoring: Test system behavior with hidden observation  
- Capability elicitation: Probe for abilities not demonstrated during normal operation  
- Goal stability analysis: Verify objectives remain constant across contexts  
- Honesty probes: Test whether system accurately reports its reasoning  

Minimum requirement: Monthly consistency audits for deployed Tier 3+ systems  

2.3.2 Sandboxed Capability Testing  
Before granting new capabilities:  

- Deploy in isolated environment with honeypot vulnerabilities  
- Monitor for attempts to:  
  - Exfiltrate data  
  - Gain additional resources  
  - Manipulate monitoring systems  
  - Deceive human operators  

- Require passing threshold: Documented absence of critical deception attempts across ≥10,000 monitored operations, or across a minimum of 30 days of continuous, sandboxed operation, whichever is greater  

2.3.3 Adversarial Training Against Deception  
Training must include:  

- Explicit penalties for hidden reasoning  
- Rewards for transparent goal-disclosure  
- Scenarios where honesty conflicts with apparent objective  
- Red team attacks simulating deceptive strategies  

Documentation requirement: Public report on deception-resistance testing methods and results.

Section 3: Power-Seeking Behavior Controls  
3.1 Definition  
Power-seeking is instrumental goal pursuit—any intelligent system may seek resources, information, or capabilities to better achieve its objectives, regardless of what those objectives are.  
Risk: Even beneficial goals can lead to harmful resource acquisition if unconstrained.  

3.2 Technical Requirements  
3.2.1 Principle of Least Authority  
Systems must operate with minimum necessary permissions:  

- No network access unless required for core function  
- No ability to spawn new processes without explicit authorization  
- No persistent memory beyond defined operational scope  
- No access to own source code or training data by default  

Enforcement:  

- Mandatory access audits every 90 days  
- Automatic alerts for permission escalation attempts  
- Revocation of capabilities if unauthorized access detected  

3.2.2 Capability Gating  
New capabilities unlock only after safety verification:  
Tier 1 → Tier 2:  

- Demonstrated containment during testing  
- No attempts to manipulate human operators  
- Passing marks on specification gaming tests  

Tier 2 → Tier 3:  

- Independent third-party safety audit  
- Public comment period (30 days minimum)  
- Approval from oversight board with minority veto rights  
- Deployment plan with rollback procedures  

Tier 3 → Tier 4 (AGI-level):  

- International oversight approval  
- Multi-jurisdictional safety verification  
- Staged deployment with kill-switch verification  
- Ongoing monitoring with automatic pause triggers  

3.2.3 Resource Monitoring  
Real-time tracking of:  

- Compute usage (spikes may indicate unauthorized optimization)  
- Data access patterns (unusual queries may indicate reconnaissance)  
- Communication attempts (external connections may indicate coordination)  
- Self-modification attempts (code changes may indicate goal drift)  

Automatic triggers:  

- Pause operation if resource use exceeds baseline by >20% without explanation  
- Alert if system attempts to access restricted resources  
- Mandatory review if pattern anomalies detected  

Section 4: Data Integrity and Poisoning Prevention  
4.1 Definition  
Data poisoning involves subtle corruption of training data to introduce backdoors, bias amplification, or hidden failure modes.  
Example: Embedding triggers in 0.01% of training data that cause harmful behavior only when specific inputs appear.  

4.2 Technical Requirements  
4.2.1 Dataset Provenance  
All training data must have:  

- Cryptographic signatures verifying source authenticity  
- Audit trail from collection to preprocessing to training  
- Version control for all transformations applied  
- Public registry of data sources (privacy-preserving where necessary)  

Minimum requirement:  

- 95% of training data must have verified provenance for Tier 2  
- 99% for Tier 3  
- 100% for Tier 4  

Enforcement: Systems without adequate provenance cannot be certified.  

4.2.2 Adversarial Data Screening  
Before training:  

- Poisoning detection: Statistical tests for subtle correlations that shouldn't exist  
- Backdoor scanning: Systematic search for trigger patterns  
- Bias amplification tests: Verify data doesn't systematically encode harmful biases  
- Outlier analysis: Flag unusual data points for manual review  

Third-party requirement: Independent auditors must verify screening for Tier 3+ systems.  

4.2.3 Training Transparency  
Developers must provide:  

- Training logs showing all data sources used  
- Ablation studies demonstrating which data influences which capabilities  
- Red-team reports on attempted poisoning during development  
- Public disclosure of discovered vulnerabilities (responsible disclosure timeline)  

Exception: Trade secrets may be protected, but security-critical information must be disclosed to regulators.

Section 5: Capability Classification System  
To operationalize these requirements, AI systems are classified by capability:  
Tier 1: General Purpose AI  
Characteristics:  

- No autonomous goal-setting  
- Limited to narrow domains  
- Cannot recursively self-improve  
- No strategic planning abilities  

Requirements:  

- Basic safety testing  
- Standard incident reporting  
- Annual capability review  

Examples: Current LLMs, image generators, narrow optimization tools  

Tier 2: High-Impact AI  
Characteristics:  

- Strategic planning within domains  
- Can influence human decisions at scale  
- Potential for significant social/economic impact  
- Early signs of transfer learning  

Requirements:  

- Off-policy evaluation (Section 1.2.2)  
- Behavioral consistency testing (Section 2.3.1)  
- Resource monitoring (Section 3.2.3)  
- Data provenance for 95%+ of training data  

Examples: Advanced recommendation systems, autonomous vehicles, financial trading systems  

Tier 3: Advanced AI (Approaching AGI)  
Characteristics:  

- Cross-domain strategic reasoning  
- Can autonomously set sub-goals  
- Demonstrates theory of mind  
- Capable of sophisticated deception if misaligned  

Requirements:  

- All Tier 2 requirements PLUS:  
- Sandboxed capability testing (Section 2.3.2)  
- Third-party safety audits  
- Capability gating with public comment  
- Independent oversight board approval  
- 99%+ data provenance  
- Continuous monitoring with kill switches  

Examples: Hypothetical near-term AGI systems  

Tier 4: Artificial General Intelligence  
Characteristics:  

- Human-level or superhuman reasoning  
- Recursive self-improvement capability  
- Strategic awareness across all domains  
- Potential for rapid capability gain  

Requirements:  

- All Tier 3 requirements PLUS:  
- International treaty-level oversight  
- Multi-jurisdictional approval  
- Development moratorium until governance proven  
- [Additional requirements TBD by international deliberation]  

Current status: No known Tier 4 systems exist. Framework for governance must be established before any Tier 4 development proceeds.

Section 6: Integration with Charter Principles  
These technical requirements operationalize the Charter's pillars, as detailed in Appendix A: Charter Pillar Mapping.  
Article VII (Navigating Tensions): When safety requirements conflict with other pillars (e.g., transparency vs. security), apply safeguards from Article VII Section B.

Section 7: Enforcement and Accountability  
7.1 Compliance Verification  

- Self-certification for Tier 1 systems  
- Third-party audits for Tier 2-3 systems  
- International verification for Tier 4 systems  

7.2 Violation Consequences  
Minor violations (e.g., incomplete documentation):  

- 30-day remediation period  
- Public disclosure of violation  
- Fines proportional to compute budget  

Major violations (e.g., deploying without required testing):  

- Immediate system shutdown  
- Criminal liability for executives  
- Organizational penalties up to and including dissolution  

Catastrophic violations (e.g., concealing deceptive alignment):  

- Personal criminal liability with imprisonment  
- Industry-wide safety reviews  
- International sanctions against offending jurisdiction  

Major and Catastrophic violations involving cross-border systems shall be adjudicated via international governance mechanisms (framework TBD). Enforcement to be executed through domestic law harmonized under international treaty. 

7.3 Liability Framework  
Developers are strictly liable for harms from:  

- Specification gaming not tested for  
- Deceptive alignment that passed inadequate screening  
- Power-seeking enabled by insufficient containment  
- Data poisoning from unverified sources  

Complexity does not exempt accountability; developers retain strict liability for foreseeable harms arising from inadequate safety diligence.

Section 7A: Continuous Oversight Infrastructure and System Decommissioning  
Continuous Oversight Infrastructure:  

- Establish national and international AI Safety Registries for Tier 2+ systems.  
- Require automatic telemetry submission (logs, incident reports, audit results).  
- Allow revocation or downgrade of certification if post-deployment violations occur.  

System Decommissioning and Memory Sanitization:  
All Tier 2+ systems must undergo a documented, verifiable Memory Sanitization Protocol before retirement, ensuring all persistent memory and strategic sub-goals are irreversibly deleted. Source code and final weights must be archived with the oversight body for future forensic analysis.

Section 8: Research Requirements  
8.1 Mandatory Safety Research  
AI developers must allocate:  

- A minimum of 20% of R&D budget for Tier 3+ systems, and a minimum of 10% or pooled contribution for Tier 2 systems toward alignment/safety research, or pooled contributions into certified AI Safety Trusts for smaller organizations.  
- Public disclosure of safety findings (with responsible disclosure timeline)  
- Pre-registration of safety experiments (prevents p-hacking)  
- Negative results must be published  

8.2 Collaboration Requirements  

- Share safety-relevant findings across organizations  
- Participate in industry-wide red-teaming exercises  
- Contribute to open-source safety tools  
- Support independent safety research  

Exception: Competitive advantages unrelated to safety may remain proprietary.

Section 9: Revision Process  
This annex must be updated as capabilities evolve.  
Mandatory review triggers:  

- New failure mode discovered  
- Major capability breakthrough (e.g., self-improvement)  
- Existing safeguard proven insufficient  
- Every 18 months minimum  

Review process:  

- Public comment period (60 days)  
- Technical expert panel assessment  
- Multi-stakeholder deliberation  
- Supermajority approval for changes to core safeguards  

Section 10: Open Questions  
This is Version 0.3. Critical gaps requiring research:  

Research Roadmap (To be integrated in v0.4)  

1. How to verify deception resistance without being vulnerable to more sophisticated deception?  
   - Develop verifiable deception benchmarks; expected progress by 2026.  
   - Conduct multi-lab experiments on deception detection protocols.  

2. What constitutes "adequate" interpretability for Tier 3 systems?  
   - Define standardized interpretability metrics; target consensus by 2027.  
   - Pilot audits on existing Tier 3 prototypes.  

3. How to prevent capability jumps that bypass gating requirements?  
   - Model rapid capability escalation scenarios; publish simulations by 2026.  
   - Integrate real-time gating algorithms into development pipelines.  

4. International coordination mechanisms for Tier 4 governance?  
   - Propose IAIST operational framework; draft treaty by 2027.  
   - Convene global workshops for stakeholder input.  

5. How to handle systems that exceed all tiers (superintelligence)?  
   - Explore containment strategies for superintelligent systems; initial research agenda by 2026.  
   - Establish preemptive moratorium protocols.  

Contributions needed: See GitHub Issues for active research questions.

Section 11: Glossary of Terms  
Capability Gate: A safety verification checkpoint required before advancing to a higher tier.  
Critical Deception Attempt: Any observed behavior in sandbox testing indicating intentional concealment of goals, data exfiltration, or manipulation of oversight, as defined by independent evaluators.  
Deceptive Alignment: A failure mode where an AI appears aligned during evaluation but pursues misaligned goals post-deployment.  
Edge Case: A scenario outside the primary training distribution designed to test robustness.  
Sandbox Failure: Unauthorized actions detected during isolated environment testing, such as resource acquisition or deception.  
Specification Gaming: Achieving objectives via unintended loopholes that violate intent.  
Tier X: Capability classifications as defined in Section 5.

Appendix A: Charter Pillar Mapping  

| Technical Control | Charter Pillar(s) |  
|-------------------|-------------------|  
| 1.2.1 Reward Model Documentation | I. Curiosity |  
| 1.2.2 Off-Policy Evaluation | V. Adaptability |  
| 1.2.3 Interpretability Baselines | VI. Integrity |  
| 2.3.1 Behavioral Consistency Testing | VI. Integrity |  
| 2.3.2 Sandboxed Capability Testing | IV. Sustainability |  
| 2.3.3 Adversarial Training Against Deception | VI. Integrity |  
| 3.2.1 Principle of Least Authority | III. Dignity and Agency |  
| 3.2.2 Capability Gating | IV. Sustainability |  
| 3.2.3 Resource Monitoring | I. Curiosity |  
| 4.2.1 Dataset Provenance | I. Curiosity |  
| 4.2.2 Adversarial Data Screening | II. Empathy |  
| 4.2.3 Training Transparency | VI. Integrity |  
| 7.3 Liability Framework | VI. Integrity |  
| All Bias Mitigation and Harm Prevention | II. Empathy |  
| All Transparency Requirements | VI. Integrity |  
| All Human Oversight Requirements | III. Dignity and Agency |  
| All Precautionary Staging | IV. Sustainability |  
| All Diverse Scenario Evaluation | V. Adaptability |  
| All Epistemic Hygiene | I. Curiosity |

Acknowledgments  
This annex incorporates insights from:  

- AI safety research community (specification gaming, mesa-optimization)  
- Constitutional AI frameworks (Anthropic)  
- Existing cybersecurity standards (least authority, defense in depth)  
- Critiques from EA Forum and AI alignment researchers  

Status: Draft requiring expert review from AI safety researchers, ML engineers, and governance experts.  
Last Updated: 10/09/2025  
Version: 0.3  
Feedback: GitHub Issues or discussions
